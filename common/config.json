{
    "model": {
        "batch_size": 16,
        "max_length_tokens": 300,
        "random_seed": 42,
        "n_classes": 2,
        "epochs": 5
    }
}
